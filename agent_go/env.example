# Environment Variables for MCP Agent
# Copy this file to .env and fill in your actual values

# =============================================================================
# LLM Provider Configuration
# =============================================================================

# AWS Bedrock (Claude models)
AWS_REGION=us-east-1
AWS_ACCESS_KEY_ID=your_aws_access_key_id
AWS_SECRET_ACCESS_KEY=your_aws_secret_access_key

# OpenAI (GPT models)
OPENAI_API_KEY=your_openai_api_key

# Anthropic (Claude models - direct API)
ANTHROPIC_API_KEY=your_anthropic_api_key

# OpenRouter (Access to multiple models)
OPEN_ROUTER_API_KEY=your_openrouter_api_key

# =============================================================================
# Model Configuration (Optional - uses defaults if not set)
# =============================================================================

# Bedrock default model
BEDROCK_PRIMARY_MODEL=us.anthropic.claude-sonnet-4-20250514-v1:0

# OpenAI default model
OPENAI_PRIMARY_MODEL=gpt-4.1-mini

# Anthropic default model
ANTHROPIC_PRIMARY_MODEL=claude-3-5-sonnet-20241022

# OpenRouter default model
OPENROUTER_PRIMARY_MODEL=moonshotai/kimi-k2

# =============================================================================
# Fallback Models (Optional - comma-separated)
# =============================================================================

# Bedrock fallback models for rate limiting (same provider)
BEDROCK_FALLBACK_MODELS=us.anthropic.claude-3-5-haiku-20241022,us.anthropic.claude-3-5-sonnet-20241022

# OpenAI fallback models for rate limiting (same provider)
OPENAI_FALLBACK_MODELS=gpt-4o-mini,gpt-3.5-turbo

# Cross-provider fallback models (Bedrock â†’ OpenAI)
# When Bedrock models fail, fall back to OpenAI models
BEDROCK_OPENAI_FALLBACK_MODELS=gpt-4o,gpt-4,gpt-3.5-turbo

# =============================================================================
# Observability (Optional)
# =============================================================================

# Langfuse tracing
TRACING_PROVIDER=langfuse
LANGFUSE_PUBLIC_KEY=your_langfuse_public_key
LANGFUSE_SECRET_KEY=your_langfuse_secret_key
LANGFUSE_DEBUG=true

# Console tracing (fallback)
# TRACING_PROVIDER=console

# No tracing
# TRACING_PROVIDER=noop

# =============================================================================
# Agent Configuration (Optional)
# =============================================================================

# Agent provider (bedrock, openai, anthropic, openrouter)
AGENT_PROVIDER=bedrock

# Agent model ID (uses provider default if not set)
AGENT_MODEL=

# Temperature for LLM responses
TEMPERATURE=0.7

# Maximum conversation turns
MAX_TURNS=20

# =============================================================================
# Cache Configuration (Optional)
# =============================================================================

# MCP Cache TTL in minutes (default: 10080 = 7 days)
MCP_CACHE_TTL_MINUTES=10080

# MCP Cache directory (default: agent_go/cache)
MCP_CACHE_DIR=

# =============================================================================
# Testing Configuration (Optional)
# =============================================================================

# Test timeout duration
TEST_TIMEOUT=5m

# Test log level
TEST_LOG_LEVEL=info

# Test log format
TEST_LOG_FORMAT=text
